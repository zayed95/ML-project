# "Analyzing Churn Prediction Dataset and Prediction of Customer Behaviour"
  our project is responsible for apply different Machine learning algorithm ( Naive Bayes , Decission tree , Support vector machine [SVM] , KNN),
  and develop predictive models using various machine learning algorithms this was about predicting the customer churn model and try optization model with high accuracy for this task .

## Table of content
  - Data set introduction 
  - ERD
  - Data cleaning
  - Data visualization
  - Naive Bayes algorithm
  - oversampling and balance data
  - Decission Tree
  - KNN algorithm
  - SVM algorithm

## Dataset introduction 
    The provided dataset is a Churn Prediction Dataset from a bank. The goal is to predict whether a customer will leave the bank (Exited = 1)
    or stay (Exited = 0). This is a supervised classification problem. The dataset contains 10,000 records with features such as customer demographics,
    financial details, and account activity.

## Features
  CreditScore: Customer's credit score.
  Geography: Country where the customer resides.
  Gender: Male or Female.
  Age: Customer's age.
  Tenure: Number of years the customer has been with the bank.
  Balance: Account balance.
  NumOfProducts: Number of products the customer has subscribed to.
  HasCrCard: Whether the customer has a credit card (1 = Yes, 0 = No).
  IsActiveMember: Whether the customer is an active member (1 = Yes, 0 = No).
  EstimatedSalary: Customer's estimated salary.
  Exited: Target variable (1 = Customer exited, 0 = Customer stayed).

## ERD 
    Inspect the dataset
    Examine unique values
    Summary Statistics

## Data cleaning  
  Drop irrelevant columns
  Check for missing values
  Check for duplicate rows
  Analyze data types
  Check for Outliers

## Data visualization   
    Histograms and Distributions
    Count Plots
    pie chart
    subplots layout
    Correlation Matrix 
    confusion Matrix 

## one hot encoder 
## Standarization (Z-score scaling)

##  Naive Bayes algorithm
    apply the algorithm 
    metrics conclusion 
    Apply Cross validation to relaible evaluation and Oversampling as the data is imbalanced
    Fine tune the Hyperparameters

## Decision Tree
    Selecting the Best Attribute
    Splitting the Dataset
    Repeat the Process
    metrics conclusion
    give insights 

## KNN 
    Select the number of neighbors (K) 
    apply the algorithm 
    split the data into (70 %  : 15% : 15% )
    compare many models 
    visulaize each model merics 
    give valuable insights 
## SVM 
    feature selection 
    Apply the algorithm 
    evaluation and Oversampling as the data is imbalanced
    Fine tune the Hyperparameters
    select best model and give metrics conclusion 
    give insights and learning curve 
    
